version: "3.9"
services:
  llama-cpp:
    image: ghcr.io/ggml-org/llama.cpp:latest
    container_name: llwb-llama-cpp
    ports:
      - "${LLAMA_CPP_HOST_PORT:-8080}:8080"
    volumes:
      - llama_data:/models
    environment:
      # llama.cpp server configuration
      LLAMA_ARG_N_THREADS: "8"
      LLAMA_ARG_N_GPU_LAYERS: "33"
    command: >
      --server
      --host 0.0.0.0
      --port 8080
      --model /models/model.gguf
      --n-threads 8
      --n-gpu-layers 33
      --ctx-size 2048
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 15s
      timeout: 10s
      retries: 20
    restart: unless-stopped

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: llwb-openwebui
    depends_on:
      - llama-cpp
    ports:
      - "${WEBUI_HOST_PORT:-3000}:8080"
    environment:
      OLLAMA_BASE_URL: "http://llama-cpp:8080"
      WEBUI_AUTH: "True"
      ENABLE_SIGNUP: "False"
      WEBUI_ADMIN_EMAIL: "${WEBUI_ADMIN_EMAIL}"
      WEBUI_ADMIN_PASSWORD: "${WEBUI_ADMIN_PASSWORD}"
    volumes:
      - openwebui_data:/app/backend/data
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O - http://localhost:8080/health | grep -q 'ok' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 20
    restart: unless-stopped

  fileapi:
    build:
      context: ../file-api
    container_name: llwb-fileapi
    environment:
      FILE_API_KEY: "${FILE_API_KEY}"
      FILE_API_ROOTS: "${FILE_API_ROOTS}"
      FILE_API_DENYLIST: "${FILE_API_DENYLIST}"
    ports:
      - "${FILE_API_HOST_PORT:-8001}:8001"
    volumes:
      - ${FILE_API_ROOTS_HOST:-C:/Users}:/mounted:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 15s
      timeout: 10s
      retries: 20
    restart: unless-stopped

volumes:
  llama_data:
  openwebui_data:
